"""preprocessing.py: Collection of classes and functions to aid in processing XML files for project."""
# David Vann (dv6bq@virginia.edu)
# DS 5001
# 5 May 2021

import os

import pandas as pd
import nltk

from bs4 import BeautifulSoup

class Document:
    """Class for handling input and parsing of XML files from Perseus Digital Library."""
    def __init__(self, path):
        """Initialize object with filepath to XML file.
        
        Args:
            path (str): filepath to XML file. 
        """
        self.path = path
        self.OHCO = ['chapter', 'para', 'sent', 'token']
        
        self.title = ''
        self.author = os.path.normpath(path).split(os.sep)[-3]
        self.text = None
        
        self.doc = None
        self.token = None
        
        playwrights = ['Aeschylus', 'Euripides', 'Sophocles']
        self.play = self.author in playwrights
        
    def parse_text_to_paras(self):
        """Parses XML document down to paragraph-like chunks using BeautifulSoup."""
        ## Parse with BeautifulSoup and extract some elements
        with open(self.path) as f:
            soup = BeautifulSoup(f, 'xml')
        self.text = soup.text
        self.title = soup.title.text
        if '.' in self.title:
            self.title = self.title.split('.')[0]
        self.title = self.title.replace('(English)', '').strip()
        
        ## Parsing tags common to many works
        # Remove footnote tags
        for note in soup.find_all('note'):
            note.replace_with('')
        
        # Replace quote tags with actual quotation marks
        for quote in soup.find_all('quote'):
            quote.replace_with(f'"{quote.text}"')
            
        # Remove placeName tags (but retain the text)
        for placeName in soup.find_all('placeName'):
            placeName.replace_with(f'{placeName.text}')
        
    
        ## Play processing
        if self.play:
            # Modify speaker tags
            for speaker in soup.find_all('speaker'):
                speaker.replace_with(f'{speaker.text}:')
            # Remove stage directions
            for stagedir in soup.find_all('stage'):
                stagedir.replace_with('')
            
        ## Poem/epic processing
        else:
            # Milestones (self-closed tags): "card" or "line" -- will use cards as section breaks (~paragraphs)
            # Line milestones only count every 5th line break (they don't actually indicate line breaks) -> replace with empty space
            for milestone in soup.find_all('milestone'):
                if milestone['unit'] == 'card':
                    milestone.replace_with('\n')
                else:
                    milestone.replace_with('')
                    
            # In the Aeneid, lines are actually designated exactly with 'l' tag; treat as sentences for consistency
            for line in soup.find_all('l'):
                line.replace_with(f'{line.text}')
                
        ## Chunk texts into chapter-type blocks
        text_list = soup.find_all('div1') # div1 tags are roughly "chapters" in most works
        
        # Loop over chapters and split into paragraph-type sections
        if self.play:
            for i, chunk in enumerate(text_list):
                text_list[i] = [speaker_chunk.text for speaker_chunk in chunk.find_all('sp')]
        else:
            for i, chunk in enumerate(text_list):
                text_list[i] = chunk.text.split('\n\n')
        
        ## Create DataFrame from paragraph strings
        self.doc = (pd.DataFrame(text_list).stack().to_frame()
                   .rename(columns={0: 'para_str'}))
        
        # Break up hyphenized words
        self.doc['para_str'] = self.doc['para_str'].str.replace('—', ' — ')
        self.doc['para_str'] = self.doc['para_str'].str.replace('-', ' - ')
        
        self.doc['para_str'] = self.doc['para_str'].str.strip()
        self.doc = self.doc[~self.doc['para_str'].str.match(r'^\s*$')] # Remove empty paragraphs (only whitespace)
        self.doc.index.names = self.OHCO[:2] # Change index names for readability
        
    def tokenize(self, remove_pos_tuple=False, remove_ws=False):
        """Tokenizes paragraphs into sentences and words.
        
        Args:
            remove_pos_tuple (bool): whether or not to remove POS tuple of (word, POS) generated by NLTK after extracting each individually
            remove_ws (bool): use an NLTK tokenizer that removes whitespace (True) or leaves them in (False)
        
        """
        self.token = (self.doc['para_str']
                   .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))
                   .stack()
                   .to_frame()
                   .rename(columns={0:'sent_str'}))
        
        # Choose tokenizer based on whitespace arg
        def word_tokenize(x):
            if remove_ws:
                s = pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))
            else:
                s = pd.Series(nltk.pos_tag(nltk.word_tokenize(x)))
            return s
        
        self.token = (self.token['sent_str']
                   .apply(word_tokenize)
                   .stack()
                   .to_frame()
                   .rename(columns={0:'pos_tuple'}))
        
        self.token['pos'] = self.token.pos_tuple.apply(lambda x: x[1])
        self.token['token_str'] = self.token.pos_tuple.apply(lambda x: x[0])
        if remove_pos_tuple:
            self.token = self.token.drop(columns='pos_tuple')

        self.token['term_str'] = self.token['token_str'].str.lower().str.replace(r'[\W_]', '', regex=True) # Normalize tokens
        self.token.index.names = self.OHCO # Rename index
        
    def pretty_print(self):
        """Displays text of document with some demarcation of sections."""
        for chap, paras in self.doc.groupby('chapter'):
            print('=====================================================================SECTION=====================================================================')
            for i, para in enumerate(paras['para_str']):
                if i != 0:
                    print('----------')
                print(para)
                
    def __repr__(self):
        return f"Document: {self.title}, by {self.author}. Play: {self.play}."

def create_tables(docs):
    """Concatenate document tables to form overall corpus tables."""
    LIB = pd.DataFrame([[idx, doc.author, doc.title, doc.path] for idx, doc in enumerate(docs)], columns=['work', 'author', 'title', 'path']).set_index('work')
    DOC = pd.concat([doc.doc for doc in docs], keys=LIB.index, names=['work'])
    TOKEN = pd.concat([doc.token for doc in docs], keys=LIB.index, names=['work'])
    return LIB, DOC, TOKEN

def extract_vocab(token):
    """Creates vocabulary from token table based on occurrence counts of each normalized term."""
    VOCAB = (token.term_str.value_counts().to_frame().sort_index().reset_index().rename(columns={'index':'term_str', 'term_str':'n'}))
    VOCAB.index.name = 'term_id'
    return VOCAB