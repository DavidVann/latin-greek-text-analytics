"""preprocessing.py: Collection of classes to aid in processing XML files for project."""
# David Vann (dv6bq@virginia.edu)
# DS 5001
# 5 May 2021

from __future__ import annotations

import os

import nltk
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from nltk.stem.porter import PorterStemmer

class Document:
    """Class for handling input and parsing of XML files from Perseus Digital Library."""
    def __init__(self, path) -> None:
        """Initialize object with filepath to XML file.
        
        Args:
            path (str): filepath to XML file. 
        """
        self.path = path
        self.OHCO = ['chapter_id', 'para_id', 'sent_id', 'token_id']
        
        self.title = ''
        self.author = os.path.normpath(path).split(os.sep)[-3]
        self.text = None
        
        self.doc = None
        self.token = None
        
        playwrights = ['Aeschylus', 'Euripides', 'Sophocles']
        self.play = self.author in playwrights
        
    def parse_text_to_paras(self) -> None:
        """Parses XML document down to paragraph-like chunks using BeautifulSoup."""
        ## Parse with BeautifulSoup and extract some elements
        with open(self.path) as f:
            soup = BeautifulSoup(f, 'xml')
        self.text = soup.text
        self.title = soup.title.text
        if '.' in self.title:
            self.title = self.title.split('.')[0]
        self.title = self.title.replace('(English)', '').strip()
        
        ## Parsing tags common to many works
        # Remove footnote tags
        for note in soup.find_all('note'):
            note.replace_with('')
        
        # Replace quote tags with actual quotation marks
        for quote in soup.find_all('quote'):
            quote.replace_with(f'"{quote.text}"')
            
        # Remove placeName tags (but retain the text)
        for placeName in soup.find_all('placeName'):
            placeName.replace_with(f'{placeName.text}')
        
    
        ## Play processing
        if self.play:
            # Modify speaker tags
            for speaker in soup.find_all('speaker'):
                speaker.replace_with(f'{speaker.text}:')
            # Remove stage directions
            for stagedir in soup.find_all('stage'):
                stagedir.replace_with('')
            
        ## Poem/epic processing
        else:
            # Milestones (self-closed tags): "card" or "line" -- will use cards as section breaks (~paragraphs)
            # Line milestones only count every 5th line break (they don't actually indicate line breaks) -> replace with empty space
            for milestone in soup.find_all('milestone'):
                if milestone['unit'] == 'card':
                    milestone.replace_with('\n')
                else:
                    milestone.replace_with('')
                    
            # In the Aeneid, lines are actually designated exactly with 'l' tag; treat as sentences for consistency
            for line in soup.find_all('l'):
                line.replace_with(f'{line.text}')
                
        ## Chunk texts into chapter-type blocks
        text_list = soup.find_all('div1') # div1 tags are roughly "chapters" in most works
        
        # Loop over chapters and split into paragraph-type sections
        if self.play:
            for i, chunk in enumerate(text_list):
                text_list[i] = [speaker_chunk.text for speaker_chunk in chunk.find_all('sp')]
        else:
            for i, chunk in enumerate(text_list):
                text_list[i] = chunk.text.split('\n\n')
        
        ## Create DataFrame from paragraph strings
        self.doc = (pd.DataFrame(text_list).stack().to_frame()
                   .rename(columns={0: 'para_str'}))
        
        # Break up hyphenized words
        self.doc['para_str'] = self.doc['para_str'].str.replace('—', ' — ')
        self.doc['para_str'] = self.doc['para_str'].str.replace('-', ' - ')
        
        self.doc['para_str'] = self.doc['para_str'].str.strip()
        self.doc = self.doc[~self.doc['para_str'].str.match(r'^\s*$')] # Remove empty paragraphs (only whitespace)
        self.doc.index.names = self.OHCO[:2] # Change index names for readability
        
    def tokenize(self, remove_pos_tuple=False, remove_ws=False) -> None:
        """Tokenizes paragraphs into sentences and words.
        
        Args:
            remove_pos_tuple (bool): whether or not to remove POS tuple of (word, POS) generated by NLTK after extracting each individually
            remove_ws (bool): use an NLTK tokenizer that removes whitespace (True) or leaves them in (False)
        
        """
        self.token = (self.doc['para_str']
                   .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))
                   .stack()
                   .to_frame()
                   .rename(columns={0:'sent_str'}))
        
        # Choose tokenizer based on whitespace arg
        def word_tokenize(x):
            if remove_ws:
                s = pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))
            else:
                s = pd.Series(nltk.pos_tag(nltk.word_tokenize(x)))
            return s
        
        self.token = (self.token['sent_str']
                   .apply(word_tokenize)
                   .stack()
                   .to_frame()
                   .rename(columns={0:'pos_tuple'}))
        
        self.token['pos'] = self.token.pos_tuple.apply(lambda x: x[1])
        self.token['token_str'] = self.token.pos_tuple.apply(lambda x: x[0])
        if remove_pos_tuple:
            self.token = self.token.drop(columns='pos_tuple')

        self.token['term_str'] = self.token['token_str'].str.lower().str.replace(r'[\W_]', '', regex=True) # Normalize tokens
        self.token.index.names = self.OHCO # Rename index
        
    def pretty_print(self, n_sections=None) -> None:
        """Displays text of document with some demarcation of sections.
        
        Args:
            n_sections (int): Number of chunks to print out. Prints entire work if None.

        """
        print(f"TITLE: {self.title}")
        print(f"AUTHOR: {self.author}")

        sections_printed = 0
        for chap, paras in self.doc.groupby('chapter_id'):
            if n_sections is not None:
                if sections_printed >= n_sections:
                    break
            print(f'=====================================================================SECTION {chap+1}=====================================================================')
            for i, para in enumerate(paras['para_str']):
                if i != 0:
                    print('----------')
                print(para)
            sections_printed += 1
                
    def __repr__(self) -> str:
        return f"Document: {self.title}, by {self.author}. Play: {self.play}."


class Corpus:
    """Container for holding corpus tables and performing further processing on a list of (processed) Documents."""
    def __init__(self, doc_list=None) -> None:
        """Extracts document tables from Documents and concatenates/converts them into LIB, DOC, and TOKEN tables.
        
        `doc_list` is only optional to allow for easier copying. It should be provided in all other cases.
        """
        self.OHCO = ['work_id', 'chapter_id', 'para_id', 'sent_id', 'token_id']

        if doc_list is not None:        
            self.lib = pd.DataFrame([[idx, d.author, d.title, d.path] for idx, d in enumerate(doc_list)], 
                                        columns=['work_id', 'author', 'title', 'path']).set_index('work_id')
            self.doc = pd.concat([d.doc for d in doc_list], keys=self.lib.index, names=['work_id'])
            self.token = pd.concat([d.token for d in doc_list], keys=self.lib.index, names=['work_id'])

            self.vocab = None

    def extract_annotate_vocab(self) -> None:
        """Extract a vocabulary and add additional linguistic features to VOCAB table (stop words, stems, etc.)."""
        self.vocab = (self.token['term_str'].value_counts().to_frame('n')
                        .reset_index().rename(columns={'index':'term_str'}))
        self.vocab.index.name = 'term_id'

        # Get list of stopwords
        stop_words = (pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])
                        .reset_index().set_index('term_str'))
        stop_words.columns = ['dummy']
        stop_words['dummy'] = 1

        # Add stopword indicator to VOCAB table
        self.vocab['stop'] = self.vocab['term_str'].map(stop_words['dummy'])
        self.vocab['stop'] = self.vocab['stop'].fillna(0).astype('int')

        # Add Porter stems to VOCAB
        stemmer = PorterStemmer()
        self.vocab['p_stem'] = self.vocab['term_str'].apply(stemmer.stem)

        # Add most common POS to VOCAB
        pos_max = self.token.groupby(['term_str', 'pos']).agg(count=('token_str', 'count'))
        pos_max = pos_max.reset_index()
        pos_max = pos_max.loc[pos_max.groupby('term_str')['count'].idxmax()]
        pos_max = pos_max.drop(columns='count')

        self.vocab = self.vocab.merge(pos_max, on='term_str').rename(columns={'pos':'pos_max'})

        self.vocab = self.vocab.set_index('term_str')

    def save_tables(self, dir: str) -> None:
        """Save corpus tables to CSV files within the given folder."""
        self.lib.to_csv(os.path.join(dir, 'LIB.csv'))
        self.doc.to_csv(os.path.join(dir, 'DOC.csv'))
        self.token.to_csv(os.path.join(dir, 'TOKEN.csv'))
        self.vocab.to_csv(os.path.join(dir, 'VOCAB.csv'))

    def load_tables(self, dir: str) -> None:
        """Load a set of tables previously computed into Corpus."""
        try:
            self.lib = pd.read_csv(os.path.join(dir, 'LIB.csv')).set_index(self.OHCO[0])
            self.doc = pd.read_csv(os.path.join(dir, 'DOC.csv')).set_index(self.OHCO[:3])
            self.token = pd.read_csv(os.path.join(dir, 'TOKEN.csv')).set_index(self.OHCO)
            self.vocab = pd.read_csv(os.path.join(dir, 'VOCAB.csv')).set_index('term_str')
        except FileNotFoundError:
            print("Missing one or more tables.")

    def compute_tfidf(self, OHCO_level=['work_id', 'chapter_id', 'para_id'], methods=['n', 'max']) -> None:
        """Computes TF-IDF values from TOKEN dataframe.
    
        Args:
            OHCO_level (list[str]): level of document organization by which to bag tokens. Based off TOKEN index columns. Defaults to paragraph-type bag.
            methods (list[str] in {'n', jp', 'cp', 'l2', 'logn', 'sub', 'max', 'bool'}: the method of computing term frequencies (TF). Can include multiple methods. 
                Defaults to 'n' and 'max' method.
                'n' -- raw sum
                'jp' -- joint probability scaling
                'cp' -- conditional probability scaling
                'l2' -- L2 normalization
                'logn' -- logarithmic (base-2) scaling
                'sub' -- sublinear scaling
                'max' -- max TF scaling
                'bool' -- boolean frequency normalization
        """
        self.tfidf_OHCO = OHCO_level

        valid_methods = ['n', 'jp', 'cp', 'l2', 'logn', 'sub', 'max', 'bool']
        for m in methods:
            if m not in valid_methods:
                return f"ERROR: '{m}' is an invalid method."

        method_funcs = {
        'jp': lambda x: x / x.sum().sum(),
        'cp': lambda x: x / x.sum(),
        'l2': lambda x: x / np.sqrt((x**2).sum()),
        'logn': lambda x: np.log2(1 + x),
        'sub': lambda x: 1 + np.log2(x),
        'max': lambda x: .4 + .6 * (x / x.max()),
        'bool': lambda x: x.astype('bool') / x.astype('bool').sum()
        }

        # Standard term frequencies
        BOW = self.token.groupby(OHCO_level + ['term_str']).term_str.count().to_frame('tf_n') # Compute bag-of-words representation of terms (term frequency)
        D = BOW.groupby(OHCO_level).tf_n # temporary reference to total term counts by bag

        # Compute other variants of term frequencies
        for m in methods:
            if m == 'n':
                continue
            else:
                BOW[f'tf_{m}'] = D.apply(method_funcs[m])
        
        self.vocab['df'] = BOW.groupby('term_str').tf_n.count() # Document frequency
        N_docs = len(D.groups)
        self.vocab['idf'] = np.log2(N_docs/self.vocab['df']) # Inverse document frequency (log2'd)

        # Compute TF-IDF
        for m in methods:
            BOW[f'tfidf_{m}'] = BOW[f'tf_{m}'] * self.vocab['idf']
        
        for m in methods:
            col = f'tfidf_{m}'
            col_sum = col + "_sum"
            self.vocab[col_sum] = BOW.groupby('term_str')[col].sum() # Sum up tf-idf values across different bags
            self.vocab[col_sum] = (self.vocab[col_sum] - self.vocab[col_sum].mean()) / self.vocab[col_sum].std() # Center + scale
            self.vocab[col_sum] = self.vocab[col_sum] - self.vocab[col_sum].min() # Subtract minimum            self.vocab[col_sum] = self.vocab[col_sum] / N_docs

        # Save BOW (important for some analysis methods)
        self.bow = BOW

    def copy(self, copy_all=True, tables=None) -> Corpus:
        """Returns a new Corpus with copied tables. Can copy all or provide list of desired tables."""
        corpus = Corpus()

        if tables is not None:
            if 'lib' in tables:
                corpus.lib = self.lib.copy()
            if 'doc' in tables:
                corpus.doc = self.doc.copy()
            if 'token' in tables:
                corpus.token = self.token.copy()
            if 'vocab' in tables:
                corpus.vocab = self.vocab.copy()
            if 'bow' in tables:
                corpus.bow = self.bow.copy()
        elif copy_all:
            corpus.lib = self.lib.copy()
            corpus.doc = self.doc.copy()
            corpus.token = self.token.copy()
            corpus.vocab = self.vocab.copy()
            corpus.bow = self.bow.copy()

        return corpus
        
