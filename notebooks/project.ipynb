{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 5001 Project Notebook: Greek and Roman Mythology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- David Vann (dv6bq@virginia.edu)\n",
    "- DS 5001\n",
    "- 5 May 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from eta_modules.preprocessing import Document, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/davidvann/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/davidvann/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/davidvann/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "\n",
    "We start by loading in the XML files for each work and parsing them to a reasonable degree with BeautifulSoup and NLTK. \n",
    "\n",
    "Since these works are all either plays or poems/epics, the concept of a \"chapter\" or \"paragraph\" doesn't translate perfectly compared to, e.g., a novel. However, the Perseus Digital Library (where these files are sourced from) has added at least top-level divisions to break up texts. In some cases, these divisions truly exist in the text (for example, *The Iliad* is broken into 24 books); in other cases, like plays, these divisions don't seem to be directly present in the text, but are akin to something like a \"scene\". I've considered all of these largest divisions as \"chapters\".\n",
    "\n",
    "To get at something like a \"paragraph\", I used a different approach based on whether the work was a play or not:\n",
    "\n",
    "- For plays, I used each speaker section (denoted by a \"\\<sp>\" in the files) as a \"paragraph\". \n",
    "- For everything else, there wasn't a built-in tag for \"paragraph\"-type divisions, but there is a self-closing \"milestone\" tag that marks the start of a new \"card\" used on the Perseus website to denote content to be displayed on one page. Since these are self-closing, they don't actually enclose the particular block of text that I wanted to get at; instead, I replaced these with newlines and split up text based on a double newline, which seemed to give fairly satisfactory results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath('..')\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "output_dir = os.path.join(data_dir, 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docpaths = glob(os.path.join(data_dir, 'raw', '**', '*.xml'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "\n",
    "for path in docpaths:\n",
    "    doc = Document(path)\n",
    "    doc_list.append(doc)\n",
    "    \n",
    "    doc.parse_text_to_paras()\n",
    "    doc.tokenize(remove_pos_tuple=True, remove_ws=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = Corpus(doc_list)\n",
    "corp.extract_vocab()\n",
    "corp.annotate_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.save_tables(os.path.join(output_dir, 'corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd06494f67c0c6cf60fb8c82a3cd67c9027a88bb126664f19bac7f5ad6badb395e4",
   "display_name": "Python 3.9.2 64-bit ('ds': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "6494f67c0c6cf60fb8c82a3cd67c9027a88bb126664f19bac7f5ad6badb395e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}